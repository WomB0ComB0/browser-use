# Enterprise Data Processing Pipeline

A modular, enterprise-grade data pipeline that monitors folders for new files and automatically generates AI-powered instructions.

## Features

- ğŸ“ **File Watching** - Real-time monitoring of data directories
- ğŸ”„ **Multi-format Support** - Process .txt, .md, .json, .csv files
- ğŸ¤– **AI Instructions** - Automatic instruction generation with Gemini
- ğŸ“Š **Structured Logging** - Production-ready logging and metrics
- âš¡ **Async Processing** - High-performance async architecture

## Quick Start

```bash
# Install dependencies
pip install -r requirements.txt

# Start the pipeline
python run_pipeline.py start

# Or process a single file
python run_pipeline.py process data/sample.txt
```

## Configuration

Edit `config.yaml` to customize:
- Input/output directories
- AI model settings
- File type filters
- Logging preferences

## Project Structure

```
pipeline/
â”œâ”€â”€ cli.py           # Command-line interface
â”œâ”€â”€ config.py        # Configuration management
â”œâ”€â”€ watcher.py       # File system watcher
â”œâ”€â”€ processor.py     # Main pipeline processor
â”œâ”€â”€ extractors/      # File type handlers
â””â”€â”€ generators/      # Instruction generators
```
